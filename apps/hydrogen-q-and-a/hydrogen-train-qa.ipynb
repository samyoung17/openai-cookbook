{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange; font-weight:bold\">Note: To answer questions based on text documents, we recommend the procedure in <a href=\"https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb\">Question Answering using Embeddings</a>. Some of the code below may rely on <a href=\"https://github.com/openai/openai-cookbook/tree/main/transition_guides_for_deprecated_API_endpoints\">deprecated API endpoints</a>.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train a fine-tuning model specialized for Q&A\n",
    "This notebook will utilize the dataset of context, question and answer pairs to additionally create adversarial questions and context pairs, where the question was not generated on that context. In those cases the model will be prompted to answer \"No sufficient context for answering the question\". We will also train a discriminator model, which predicts whether the question can be answered based on the context or not.\n",
    "\n",
    "We will add hard adversarial examples as well, which will be based either on semantically similar sections, or neighbouring sections, originating from the same article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                   heading                                            content  \\\n0            ## Glossary\\n  - **Asset:** An item that is involved in the g...   \n1  ## Domain information\\n  - The amount of hydrogen that can be dispensed...   \n2      ## Useful reading\\n  - Renewable Energy Directive and it's relation...   \n3   ## CPH2 electrolyser\\n  **Control system contact:** [ryan@davisnetsyst...   \n4    ## GHS electrolyser\\n  **Control system contact:** [mko@greenhydrogen...   \n\n                                           page_name               title  \\\n0         Domain 737d38c5704b40009666d5a2e4ebd0a2.md          # Domain\\n   \n1         Domain 737d38c5704b40009666d5a2e4ebd0a2.md          # Domain\\n   \n2         Domain 737d38c5704b40009666d5a2e4ebd0a2.md          # Domain\\n   \n3  Hardware specs 70c7375fb9cc4638acacafdd6aa304f...  # Hardware specs\\n   \n4  Hardware specs 70c7375fb9cc4638acacafdd6aa304f...  # Hardware specs\\n   \n\n                                             context  n_tokens_before  tokens  \\\n0  ## Glossary\\n- **Asset:** An item that is invo...             1232    1232   \n1  ## Domain information\\n- The amount of hydroge...              177     177   \n2  ## Useful reading\\n- Renewable Energy Directiv...              542     542   \n3  ## CPH2 electrolyser\\n**Control system contact...              543     543   \n4  ## GHS electrolyser\\n**Control system contact:...              491     491   \n\n                                           questions  \\\n0  1. What is an asset in the context of hydrogen...   \n1  1. What is the pressure that is required by th...   \n2  1. What is the Renewable Energy Directive?\\n2....   \n3  1. What is the control system for the electrol...   \n4  1. What is the make and model of the PLC?\\n2. ...   \n\n                                             answers  \n0  1. An asset in the context of hydrogen generat...  \n1  1. The pressure that is required by the off-ta...  \n2  1. The Renewable Energy Directive is a directi...  \n3  1. The control system for the electrolyser is ...  \n4  1. The make and model of the PLC is not yet kn...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>heading</th>\n      <th>content</th>\n      <th>page_name</th>\n      <th>title</th>\n      <th>context</th>\n      <th>n_tokens_before</th>\n      <th>tokens</th>\n      <th>questions</th>\n      <th>answers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>## Glossary\\n</td>\n      <td>- **Asset:** An item that is involved in the g...</td>\n      <td>Domain 737d38c5704b40009666d5a2e4ebd0a2.md</td>\n      <td># Domain\\n</td>\n      <td>## Glossary\\n- **Asset:** An item that is invo...</td>\n      <td>1232</td>\n      <td>1232</td>\n      <td>1. What is an asset in the context of hydrogen...</td>\n      <td>1. An asset in the context of hydrogen generat...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>## Domain information\\n</td>\n      <td>- The amount of hydrogen that can be dispensed...</td>\n      <td>Domain 737d38c5704b40009666d5a2e4ebd0a2.md</td>\n      <td># Domain\\n</td>\n      <td>## Domain information\\n- The amount of hydroge...</td>\n      <td>177</td>\n      <td>177</td>\n      <td>1. What is the pressure that is required by th...</td>\n      <td>1. The pressure that is required by the off-ta...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>## Useful reading\\n</td>\n      <td>- Renewable Energy Directive and it's relation...</td>\n      <td>Domain 737d38c5704b40009666d5a2e4ebd0a2.md</td>\n      <td># Domain\\n</td>\n      <td>## Useful reading\\n- Renewable Energy Directiv...</td>\n      <td>542</td>\n      <td>542</td>\n      <td>1. What is the Renewable Energy Directive?\\n2....</td>\n      <td>1. The Renewable Energy Directive is a directi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>## CPH2 electrolyser\\n</td>\n      <td>**Control system contact:** [ryan@davisnetsyst...</td>\n      <td>Hardware specs 70c7375fb9cc4638acacafdd6aa304f...</td>\n      <td># Hardware specs\\n</td>\n      <td>## CPH2 electrolyser\\n**Control system contact...</td>\n      <td>543</td>\n      <td>543</td>\n      <td>1. What is the control system for the electrol...</td>\n      <td>1. The control system for the electrolyser is ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>## GHS electrolyser\\n</td>\n      <td>**Control system contact:** [mko@greenhydrogen...</td>\n      <td>Hardware specs 70c7375fb9cc4638acacafdd6aa304f...</td>\n      <td># Hardware specs\\n</td>\n      <td>## GHS electrolyser\\n**Control system contact:...</td>\n      <td>491</td>\n      <td>491</td>\n      <td>1. What is the make and model of the PLC?\\n2. ...</td>\n      <td>1. The make and model of the PLC is not yet kn...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "openai.api_key_path = '/Users/samyoung/.openai-api-key.txt'\n",
    "os.environ['OPENAI_API_KEY'] = open(openai.api_key_path).read()\n",
    "\n",
    "df = pd.read_csv('results/hydrogen_q_and_a.csv')\n",
    "hydrogen_search_fileid = \"file-XIYZbCG5ODNJKdAAcYnx75fK\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the sections into a training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(44, 12)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we check that the separator we intend to use isn't present within the contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.context.str.contains('->').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create the fine-tuning datasets for Q&A and discriminator models\n",
    "The fine-tuning dataset is created in the following way. For every corresponding question, answer and context pair we create:\n",
    "- Positive example: correct question, answer, context pair\n",
    "- Negative examples:\n",
    "  - random negative example, where the random context is paired with the question \n",
    "  - two hard negative examples\n",
    "    - one originating from the same wikipedia article\n",
    "    - another, which is most similar to the correct context\n",
    "\n",
    "This process is noisy, as sometimes the question might be answerable given a different context, but on average we hope this won't affect the peformance too much.\n",
    "\n",
    "We apply the same process of dataset creation for both the discriminator, and the Q&A answering model. We apply the process separately for the training and testing set, to ensure that the examples from the training set don't feature within the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_random_similar_contexts(question, context, file_id=hydrogen_search_fileid, search_model='ada', max_rerank=10):\n",
    "    \"\"\"\n",
    "    Find similar contexts to the given context using the search file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = openai.Engine(search_model).search(\n",
    "            search_model=search_model, \n",
    "            query=question, \n",
    "            max_rerank=max_rerank,\n",
    "            file=file_id\n",
    "        )\n",
    "        candidates = []\n",
    "        for result in results['data'][:3]:\n",
    "            if result['text'] == context:\n",
    "                continue\n",
    "            candidates.append(result['text'])\n",
    "        random_candidate = random.choice(candidates)\n",
    "        return random_candidate\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\"\n",
    "\n",
    "def create_fine_tuning_dataset(df, discriminator=False, n_negative=1, add_related=False):\n",
    "    \"\"\"\n",
    "    Create a dataset for fine tuning the OpenAI model; either for a discriminator model, \n",
    "    or a model specializing in Q&A, where it says if no relevant context is found.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        The dataframe containing the question, answer and context pairs\n",
    "    discriminator: bool\n",
    "        Whether to create a dataset for the discriminator\n",
    "    n_negative: int\n",
    "        The number of random negative samples to add (using a random context)\n",
    "    add_related: bool\n",
    "        Whether to add the related contexts to the correct context. These are hard negative examples\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The dataframe containing the prompts and completions, ready for fine-tuning\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for i, row in df.iterrows():\n",
    "        for q, a in zip((\"1.\" + row.questions).split('\\n'), (\"1.\" + row.answers).split('\\n')):\n",
    "            if len(q) >10 and len(a) >10:\n",
    "                if discriminator:\n",
    "                    rows.append({\"prompt\":f\"{row.context}\\nQuestion: {q[2:].strip()}\\n Related:\", \"completion\":f\" yes\"})\n",
    "                else:\n",
    "                    rows.append({\"prompt\":f\"{row.context}\\nQuestion: {q[2:].strip()}\\nAnswer:\", \"completion\":f\" {a[2:].strip()}\"})\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        for q in (\"1.\" + row.questions).split('\\n'):\n",
    "            if len(q) >10:\n",
    "                for j in range(n_negative + (2 if add_related else 0)):\n",
    "                    random_context = \"\"\n",
    "                    if j == 0 and add_related:\n",
    "                        # add the related contexts based on originating from the same wikipedia page\n",
    "                        subset = df[(df.title == row.title) & (df.context != row.context)]\n",
    "                        \n",
    "                        if len(subset) < 1:\n",
    "                            continue\n",
    "                        random_context = subset.sample(1).iloc[0].context\n",
    "                    if j == 1 and add_related:\n",
    "                        # add the related contexts based on the most similar contexts according to the search\n",
    "                        random_context = get_random_similar_contexts(q[2:].strip(), row.context, search_model='ada', max_rerank=10)\n",
    "                    else:\n",
    "                        while True:\n",
    "                            # add random context, which isn't the correct context\n",
    "                            random_context = df.sample(1).iloc[0].context\n",
    "                            if random_context != row.context:\n",
    "                                break\n",
    "                    if discriminator:\n",
    "                        rows.append({\"prompt\":f\"{random_context}\\nQuestion: {q[2:].strip()}\\n Related:\", \"completion\":f\" no\"})\n",
    "                    else:\n",
    "                        rows.append({\"prompt\":f\"{random_context}\\nQuestion: {q[2:].strip()}\\nAnswer:\", \"completion\":f\" No appropriate context found to answer the question.\"})\n",
    "\n",
    "    return pd.DataFrame(rows) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the same process of dataset creation for both the discriminator, and the Q&A answering model. We apply the process separately for the training and testing set, to ensure that the examples from the training set don't feature within the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n"
     ]
    }
   ],
   "source": [
    "for name, is_disc in [('discriminator', True), ('qa', False)]:\n",
    "    for train_test, dt in [('train', train_df), ('test', test_df)]:\n",
    "        ft = create_fine_tuning_dataset(dt, discriminator=is_disc, n_negative=1, add_related=True)\n",
    "        ft.to_json(f'results/{name}_{train_test}.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We formatted the data according to the recommendations from the fine-tuning tool, which is available using\n",
    "> openai tools fine_tunes.prepare_data -f qa_train.jsonl\n",
    "\n",
    "We highly recommend that you use this tool, which suggests improvements in your data formatting for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Submit the datasets for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|███████████████████████| 276k/276k [00:00<00:00, 189Mit/s]\r\n",
      "Uploaded file from results/discriminator_train.jsonl: file-NmaN6Xf9qu5ob981OM5v44l6\r\n",
      "Upload progress: 100%|███████████████████████| 154k/154k [00:00<00:00, 105Mit/s]\r\n",
      "Uploaded file from results/discriminator_test.jsonl: file-mlTRKLhVUuFfhjSXG6Awxhpc\r\n",
      "Created fine-tune: ft-iTPwcMMRjJGZJdlbCFiBc38I\r\n",
      "Streaming events until fine-tuning is complete...\r\n",
      "\r\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\r\n",
      "[2023-05-23 10:42:04] Created fine-tune: ft-iTPwcMMRjJGZJdlbCFiBc38I\r\n",
      "\r\n",
      "Stream interrupted (client disconnected).\r\n",
      "To resume the stream, run:\r\n",
      "\r\n",
      "  openai api fine_tunes.follow -i ft-iTPwcMMRjJGZJdlbCFiBc38I\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.create -t \"results/discriminator_train.jsonl\" -v \"results/discriminator_test.jsonl\" --batch_size 16  --compute_classification_metrics --classification_positive_class \" yes\" --model ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|███████████████████████| 320k/320k [00:00<00:00, 263Mit/s]\r\n",
      "Uploaded file from results/qa_train.jsonl: file-XIYZbCG5ODNJKdAAcYnx75fK\r\n",
      "Upload progress: 100%|███████████████████████| 163k/163k [00:00<00:00, 176Mit/s]\r\n",
      "Uploaded file from results/qa_test.jsonl: file-K4Roi3UUgl3Q1NUtRZcSQ3LL\r\n",
      "Created fine-tune: ft-hWvJUVimH86jazkFweyhTESv\r\n",
      "Streaming events until fine-tuning is complete...\r\n",
      "\r\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\r\n",
      "[2023-05-23 10:43:58] Created fine-tune: ft-hWvJUVimH86jazkFweyhTESv\r\n",
      "\r\n",
      "Stream interrupted (client disconnected).\r\n",
      "To resume the stream, run:\r\n",
      "\r\n",
      "  openai api fine_tunes.follow -i ft-hWvJUVimH86jazkFweyhTESv\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.create -t \"results/qa_train.jsonl\" -v \"results/qa_test.jsonl\" --batch_size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Using the fine-tuned models\n",
    "\n",
    "We will now use the fine-tuned discriminator and the fine-tuned Q&A model. By requesting logprobs, we can see how certain the discriminator is in a `yes` vs `no` answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<OpenAIObject at 0x15439b010> JSON: {\n   \" no\": -7.395886,\n   \" yes\": -0.0013778893\n }]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_discriminator = \"ada:ft-krakenflex-2023-05-23-14-23-15\"\n",
    "ft_qa = \"curie:ft-krakenflex-2023-05-23-14-26-47\"\n",
    "\n",
    "def apply_ft_discriminator(context, question, discriminator_model):\n",
    "    \"\"\"\n",
    "    Apply the fine tuned discriminator to a question, to assess whether it can be answered from the context.\n",
    "    \"\"\"\n",
    "    prompt = f\"{context}\\nQuestion: {question}\\n Related:\"\n",
    "    result = openai.Completion.create(model=discriminator_model, prompt=prompt, max_tokens=1, temperature=0, top_p=1, n=1, logprobs=2)\n",
    "    return result['choices'][0]['logprobs']['top_logprobs']\n",
    "\n",
    "apply_ft_discriminator('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.', \n",
    "                        'What was the first human-made object in space?', ft_discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model can generalize well to different contexts and questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "' The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957'"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_ft_qa_answer(context, question, answering_model):\n",
    "    \"\"\"\n",
    "    Apply the fine tuned discriminator to a question\n",
    "    \"\"\"\n",
    "    prompt = f\"{context}\\nQuestion: {question}\\nAnswer:\"\n",
    "    result = openai.Completion.create(model=answering_model, prompt=prompt, max_tokens=30, temperature=0, top_p=1, n=1, stop=['.','\\n'])\n",
    "    return result['choices'][0]['text']\n",
    "\n",
    "apply_ft_qa_answer('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.', \n",
    "                    'What was the first human-made object in space?', ft_qa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model can answer the question, when the context is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "' No appropriate context found to answer the question'"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_ft_qa_answer('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.',\n",
    "                    'What is impressive about the Soviet Union?', ft_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "' No appropriate context found to answer the question'"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_ft_qa_answer('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.',\n",
    "                    'How many cars were produced in the Soviet Union in 1970?', ft_qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model knows when to answer the question, and when to say that insufficient context is present to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine a discriminator and a base model, or a fine-tuned Q&A model. Discriminator can essentially serve as a decision whether the question can be answered given the context or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "' No appropriate context found to answer the question'"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_question_conditionally(answering_model, discriminator_model, context, question, discriminator_logprob_yes_modifier=0):\n",
    "    logprobs = apply_ft_discriminator(context, question, discriminator_model)\n",
    "    yes_logprob = logprobs[' yes'] if ' yes' in logprobs else -100\n",
    "    no_logprob = logprobs[' no'] if ' no' in logprobs else -100\n",
    "    if yes_logprob + discriminator_logprob_yes_modifier < no_logprob:\n",
    "        return \" No appropriate context found to answer the question based on the discriminator.\"\n",
    "    return apply_ft_qa_answer(context, question, answering_model)\n",
    "answer_question_conditionally(ft_qa, ft_discriminator, \n",
    "                                \"Crowdless games are a rare although not unheard-of occurrence in sports. \\\n",
    "                                 When they do occur, it is usually the result of events beyond the control \\\n",
    "                                 of the teams or fans, such as weather-related concerns, public health concerns, \\\n",
    "                                 or wider civil disturbances unrelated to the game. For instance, \\\n",
    "                                 the COVID-19 pandemic caused many sports leagues around the world \\\n",
    "                                 to be played behind closed doors.\",\n",
    "                                \"Could weather cause a sport event to have no crowd?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function illustrates how to potentially combine a discriminator and a fine-tuned Q&A model. This gives a more fine-grained control over how certain we want the model to be before it answers the question.\n",
    "\n",
    "We'll now take a look on how answers endpoint works - combining search to retrieve the relevant context from a knowledge base, and then using the fine-tuned Q&A model to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Answering the question based on a knowledge base\n",
    "Finally we can use a logic similar to the [/answers](https://beta.openai.com/docs/api-reference/answers) endpoint, where we first search for the relevant context, and then ask a Q&A model to answer the question given that context. If you'd like to see the implementation details, check out the [`answers_with_ft.py`](answers_with_ft.py) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "search",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/code/openai-cookbook/venv/lib/python3.10/site-packages/openai/openai_object.py:59\u001B[0m, in \u001B[0;36mOpenAIObject.__getattr__\u001B[0;34m(self, k)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[0;31mKeyError\u001B[0m: 'search'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01manswers_with_ft\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m answer_question\n\u001B[0;32m----> 2\u001B[0m \u001B[43manswer_question\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhydrogen_search_fileid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mft_qa\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mWhat is the maximum compression of a Maximator compressor?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/code/openai-cookbook/apps/hydrogen-q-and-a/answers_with_ft.py:65\u001B[0m, in \u001B[0;36manswer_question\u001B[0;34m(search_file_id, fine_tuned_qa_model, question, max_len, search_model, max_rerank, debug, stop_sequence, max_tokens)\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21manswer_question\u001B[39m(\n\u001B[1;32m     42\u001B[0m     search_file_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<SEARCH_FILE_ID>\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     43\u001B[0m     fine_tuned_qa_model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<FT_QA_MODEL_ID>\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     50\u001B[0m     max_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m,\n\u001B[1;32m     51\u001B[0m ):\n\u001B[1;32m     52\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;124;03m    Answer a question based on the most similar context from the search file, using your fine-tuned model.\u001B[39;00m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;124;03m    :param question: The question\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;124;03m    :return: The answer\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 65\u001B[0m     context \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_context\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     67\u001B[0m \u001B[43m        \u001B[49m\u001B[43msearch_file_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     68\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     69\u001B[0m \u001B[43m        \u001B[49m\u001B[43msearch_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msearch_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_rerank\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_rerank\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     71\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m debug:\n\u001B[1;32m     73\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mContext:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m context)\n",
      "File \u001B[0;32m~/code/openai-cookbook/apps/hydrogen-q-and-a/answers_with_ft.py:24\u001B[0m, in \u001B[0;36mcreate_context\u001B[0;34m(question, search_file_id, max_len, search_model, max_rerank)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_context\u001B[39m(\n\u001B[1;32m     13\u001B[0m     question, search_file_id, max_len\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1800\u001B[39m, search_model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mada\u001B[39m\u001B[38;5;124m\"\u001B[39m, max_rerank\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m\n\u001B[1;32m     14\u001B[0m ):\n\u001B[1;32m     15\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;124;03m    Create a context for a question by finding the most similar context from the search file.\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03m    :param question: The question\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;124;03m    :return: The context\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[43mopenai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEngine\u001B[49m\u001B[43m(\u001B[49m\u001B[43msearch_model\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msearch\u001B[49m(\n\u001B[1;32m     25\u001B[0m         search_model\u001B[38;5;241m=\u001B[39msearch_model,\n\u001B[1;32m     26\u001B[0m         query\u001B[38;5;241m=\u001B[39mquestion,\n\u001B[1;32m     27\u001B[0m         max_rerank\u001B[38;5;241m=\u001B[39mmax_rerank,\n\u001B[1;32m     28\u001B[0m         file\u001B[38;5;241m=\u001B[39msearch_file_id,\n\u001B[1;32m     29\u001B[0m         return_metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     30\u001B[0m     )\n\u001B[1;32m     31\u001B[0m     returns \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     32\u001B[0m     cur_len \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/code/openai-cookbook/venv/lib/python3.10/site-packages/openai/openai_object.py:61\u001B[0m, in \u001B[0;36mOpenAIObject.__getattr__\u001B[0;34m(self, k)\u001B[0m\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m[k]\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m---> 61\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;241m*\u001B[39merr\u001B[38;5;241m.\u001B[39margs)\n",
      "\u001B[0;31mAttributeError\u001B[0m: search"
     ]
    }
   ],
   "source": [
    "from answers_with_ft import answer_question\n",
    "answer_question(hydrogen_search_fileid, ft_qa, \"What is the maximum compression of a Maximator compressor?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "'file-XIYZbCG5ODNJKdAAcYnx75fK'"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydrogen_search_fileid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('3.9.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb9817b186a29e4e9713184d901f26c1ee05ad25243d878baff7f31bb1fef480"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
